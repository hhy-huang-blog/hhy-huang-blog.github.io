---
title: CMSC5724_Lec1
date: 2024-09-18 14:43:55
tags: Course
---

## Generation Theorem
这里关注的是，对于sample训练集的error和整体数据或者说general的error之间的slack，也就是关注overfitting发生的可能性，我们希望这个slack越小越好，这样就可以置信度尽可能高地由{% katex %}error_{\mathbb{S}}(h){% endkatex %}预测{% katex %}error_{\mathbb{G}}(h){% endkatex %}，其中{% katex %}h{% endkatex %}是当前的classifier。

For the following words, "True" with probability at least {% katex %}1 - \delta{% endkatex %}, which is confident:

{% katex %}
error_{\mathbb{G}}(h) \leqslant error_{\mathbb{S}}(h) + \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}}
{% endkatex %}

其中{% katex %}\mathbb{H}{% endkatex %}指的是classifier的全集，含义是，当我sample的训练集越大，那么slack就会越小，我越可以用训练集的error估计整体分布的error。当我的classifier全集的数量越多，或者说函数参数量越多，slack越大，overfitting越容易发生。此外{% katex %}\delta{% endkatex %}越小，上面公式置信度越高，slack就越大。

## Q: 如何确定Sample Size
下面给出一个场景：让{% katex %}\mathbb{H}{% endkatex %}表示拥有k个叶子节点的所有决策树集合，也就是{% katex %}2k - 1{% endkatex %}个节点。

那么{% katex %}|\mathbb{H}|{% endkatex %}是多少，我们假设一个node是2个words，然后在内存空间占用128bits，那么满足这个条件的所有的决策树，一定在{% katex %}128(2k - 1){% endkatex %}这个内存空间内，然后每个bit都有两个可能的状态，有数据或者没有数据，所以{% katex %}|\mathbb{H}|{% endkatex %}也就是所有决策树的possibles一共有{% katex %}2^{128(2k - 1)}{% endkatex %}。这里我们令：

{% katex %}
l \leftarrow 128(2k - 1)
{% endkatex %}
<b></b>
{% katex %}
|\mathbb{H}| \leqslant 2^{l}
{% endkatex %}

所以就有
{% katex %}
slack = \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}} \ 
{% endkatex %}
