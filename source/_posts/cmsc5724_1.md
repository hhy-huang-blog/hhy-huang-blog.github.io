---
title: CMSC5724_Lec1
date: 2024-09-18 14:43:55
tags: Course
---

## Generation Theorem
这里关注的是，对于sample训练集的error和整体数据或者说general的error之间的slack，也就是关注overfitting发生的可能性，我们希望这个slack越小越好，这样就可以置信度尽可能高地由{% katex %}error_{\mathbb{S}}(h){% endkatex %}预测{% katex %}error_{\mathbb{G}}(h){% endkatex %}，其中{% katex %}h{% endkatex %}是当前的classifier。

For the following words, "True" with probability at least {% katex %}1 - \delta{% endkatex %}, which is confident:

{% katex %}
error_{\mathbb{G}}(h) \leqslant error_{\mathbb{S}}(h) + \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}}
{% endkatex %}

其中{% katex %}\mathbb{H}{% endkatex %}指的是classifier的全集，含义是，当我sample的训练集越大，那么slack就会越小，我越可以用训练集的error估计整体分布的error。当我的classifier全集的数量越多，或者说函数参数量越多，slack越大，overfitting越容易发生。此外{% katex %}\delta{% endkatex %}越小，上面公式置信度越高，slack就越大。

## Q: 如何确定Sample Size
下面给出一个场景：让{% katex %}\mathbb{H}{% endkatex %}表示拥有k个叶子节点的所有决策树集合，也就是{% katex %}2k - 1{% endkatex %}个节点。

那么{% katex %}|\mathbb{H}|{% endkatex %}是多少，我们假设一个node是2个words，然后在内存空间占用128bits，那么满足这个条件的所有的决策树，一定在{% katex %}128(2k - 1){% endkatex %}这个内存空间内，然后每个bit都有两个可能的状态，有数据或者没有数据，所以{% katex %}|\mathbb{H}|{% endkatex %}也就是所有决策树的possibles一共有{% katex %}2^{128(2k - 1)}{% endkatex %}。这里我们令：

{% katex %}
l \leftarrow 128(2k - 1)
{% endkatex %}
<b></b>
{% katex %}
|\mathbb{H}| \leqslant 2^{l}
{% endkatex %}

所以就有，当我们希望slack少于1%时：
{% katex %}
slack = \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}} \approx \sqrt{\frac{l}{2|\mathbb{S}|}} \rightarrow 0.01
{% endkatex %}
<b></b>
{% katex %}
|\mathbb{S}| = \frac{10000}{2}l
{% endkatex %}

其中l就是所有当前限制下分类器的总可能数量，也可以理解为参数量。

## Go to think about the scenario of Deep Learning

当我们训练集的数据量很小的时候，也就是{% katex %}|\mathbb{S}|{% endkatex %}很小的时候，同时我们又使用了一个参数量很大的神经网络模型，也就是{% katex %}|\mathbb{H}|{% endkatex %}很大的时候。可以发现slack就会变得非常大，所以overfitting就会发生。这也是为什么曾经在用于训练的标注数据较少的时候，不曾出现如今的较大参数量的神经网络模型，或者说DL并没有现如今这么有效。

## Union Bound
给n个IID（独立同分布）随机变量
{% katex %}
A_1, A_2, A_3...A_n
{% endkatex %}
<b></b>
{% katex %}
Pr(A_1 \cup A_2 .. \cup A_n) \leqslant \sum^{n}_{i=1}Pr(A_i)
{% endkatex %}

## Hoeffding Bound
同样给出服从伯努利分布的n个IID随机变量
{% katex %}
X_1, X_2, X_3...X_n
{% endkatex %}
那么显然有：
{% katex %}
t = \sum^{n}_{i=1}X_i
{% endkatex %}
<b></b>
{% katex %}
E(\frac{t}{n}) = p
{% endkatex %}
也就是样本的均值的期望等于整体分布的期望。

于是给出：
{% katex %}
Pr(\frac{t}{n} < p - \alpha) \leqslant e^{-2n \alpha^{2}}
{% endkatex %}
<b></b>
{% katex %}
Pr(\frac{t}{n} > p - \alpha) \leqslant e^{-2n \alpha^{2}}
{% endkatex %}

这个边界给出了一个事实，也就是当我样本均值的期望离整体分布的期望越来越远的时候，这个事情（这个bad event）发生的概率会越来越小。

## Go back to the scenario of Classifier
下面把Hoeffding Bound应用在分类器的场景下。

这里采样n个样本对:
{% katex %}
O_1, O_2, .. , O_n
{% endkatex %}
每一个样本对是{% katex %}(x_i, y_i){% endkatex %}

然后我们再定一个{% katex %}X_i{% endkatex %}，服从下面的分布。

{% katex %}
X_i = \left\{\begin{matrix}
 1& if h(x_i) == y_i  \\
 0& else  \\
\end{matrix}\right.
{% endkatex %}

所以能算出sample的error：
{% katex %}
error_{\mathbb{S}}(h) = \frac{1}{n} \sum^{n}_{i = 1}X_i
{% endkatex %}

然后我们希望的是{% katex %}error_{\mathbb{S}}(h){% endkatex %}能够尽可能地接近general error，也就是在整体数据上的损失。也就是我们希望sample出来数据的error能够准确地体现整体数据的error。

所以当二者发生{% katex %}\alpha{% endkatex %}的偏移的时候，这也是个bad event，当这个偏移越大，这个bad event发生的概率越小。所以就可以用到hoeffding bound。
{% katex %}
Pr[error_{\mathbb{S}}(h) > error_{\mathbb{G}}(h) + \alpha] \leqslant e^{-2n\alpha^{2}}
{% endkatex %}
<br></br>
{% katex %}
Pr[error_{\mathbb{S}}(h) < error_{\mathbb{G}}(h) - \alpha] \leqslant e^{-2n\alpha^{2}}
{% endkatex %}


