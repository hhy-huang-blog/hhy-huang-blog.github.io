---
title: CMSC5724_Lec1
date: 2024-09-18 14:43:55
tags: Course
---

## Generation Theorem
这里关注的是，对于sample训练集的error和整体数据或者说general的error之间的slack，也就是关注overfitting发生的可能性，我们希望这个slack越小越好，这样就可以置信度尽可能高地由{% katex %}error_{\mathbb{S}}(h){% endkatex %}预测{% katex %}error_{\mathbb{G}}(h){% endkatex %}，其中{% katex %}h{% endkatex %}是当前的classifier。

For the following words, "True" with probability at least {% katex %}1 - \delta{% endkatex %}, which is confident:

{% katex %}
error_{\mathbb{G}}(h) \leqslant error_{\mathbb{S}}(h) + \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}}
{% endkatex %}

其中{% katex %}\mathbb{H}{% endkatex %}指的是classifier的全集，含义是，当我sample的训练集越大，那么slack就会越小，我越可以用训练集的error估计整体分布的error。当我的classifier全集的数量越多，或者说函数参数量越多，slack越大，overfitting越容易发生。此外{% katex %}\delta{% endkatex %}越小，上面公式置信度越高，slack就越大。

## Q: 如何确定Sample Size
下面给出一个场景：让{% katex %}\mathbb{H}{% endkatex %}表示拥有k个叶子节点的所有决策树集合，也就是{% katex %}2k - 1{% endkatex %}个节点。

那么{% katex %}|\mathbb{H}|{% endkatex %}是多少，我们假设一个node是2个words，然后在内存空间占用128bits，那么满足这个条件的所有的决策树，一定在{% katex %}128(2k - 1){% endkatex %}这个内存空间内，然后每个bit都有两个可能的状态，有数据或者没有数据，所以{% katex %}|\mathbb{H}|{% endkatex %}也就是所有决策树的possibles一共有{% katex %}2^{128(2k - 1)}{% endkatex %}。这里我们令：

{% katex %}
l \leftarrow 128(2k - 1)
{% endkatex %}
<b></b>
{% katex %}
|\mathbb{H}| \leqslant 2^{l}
{% endkatex %}

所以就有，当我们希望slack少于1%时：
{% katex %}
slack = \sqrt{\frac{ln(1 / \delta) + ln(\mathbb{H})}{2|\mathbb{S}|}} \approx \sqrt{\frac{l}{2|\mathbb{S}|}} \rightarrow 0.01
{% endkatex %}
<b></b>
{% katex %}
|\mathbb{S}| = \frac{10000}{2}l
{% endkatex %}

其中l就是所有当前限制下分类器的总可能数量，也可以理解为参数量。

## Go to think about the scenario of Deep Learning

当我们训练集的数据量很小的时候，也就是{% katex %}|\mathbb{S}|{% endkatex %}很小的时候，同时我们又使用了一个参数量很大的神经网络模型，也就是{% katex %}|\mathbb{H}|{% endkatex %}很大的时候。可以发现slack就会变得非常大，所以overfitting就会发生。这也是为什么曾经在用于训练的标注数据较少的时候，不曾出现如今的较大参数量的神经网络模型，或者说DL并没有现如今这么有效。

## Union Bound
给n个IID（独立同分布）随机变量
{% katex %}
A_1, A_2, A_3...A_n
{% endkatex %}
<b></b>
{% katex %}
Pr(A_1 \cup A_2 .. \cup A_n) \leqslant \sum^{n}_{i=1}Pr(A_i)
{% endkatex %}

## Hoeffding Bound
同样给出服从伯努利分布的n个IID随机变量
{% katex %}
X_1, X_2, X_3...X_n
{% endkatex %}
那么显然有：
{% katex %}
t = \sum^{n}_{i=1}X_i
{% endkatex %}
<b></b>
{% katex %}
E(\frac{t}{n}) = p
{% endkatex %}
也就是样本的均值的期望等于整体分布的期望。

于是给出：
{% katex %}
Pr(\frac{t}{n} < p - \alpha) \leqslant e^{-2n \alpha^{2}}
{% endkatex %}